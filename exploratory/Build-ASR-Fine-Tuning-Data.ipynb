{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5025b33a",
   "metadata": {},
   "source": [
    "# ASR Fine Tuning\n",
    "\n",
    "In this notebook I will record myself reading pieces of text so that I can evaluate ASR on my voice and build some fine tuning data for ASR also."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aabb43ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from glob import glob\n",
    "import librosa\n",
    "import ipywidgets\n",
    "import IPython.display as ipd\n",
    "import numpy as np\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import string\n",
    "from tqdm.notebook import tqdm\n",
    "import wave\n",
    "\n",
    "# For accessing transcribers later\n",
    "sys.path.append(os.path.dirname(os.getcwd()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03d6fc0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('ASR_Sentences.txt') as f:\n",
    "    text_sents = [line.strip() for line in f]\n",
    "len(text_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ceff106",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab09b8572f0479fa93e011a279f5a96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Textarea(value='Hello', description='Sentence:', layout=Layout(height='100px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfbafda91ef445a990123f9752c0723a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Start Recording', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b655690518324baaafe7a477dec0b896",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Stop Recording', disabled=True, style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ef46590f0ac4c969732e9007c644063",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Next', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "import IPython.display as ipd\n",
    "import ipywidgets as widgets\n",
    "import threading\n",
    "\n",
    "recording_thread = None\n",
    "stream = None\n",
    "def record():\n",
    "    global stream\n",
    "    while recording:\n",
    "        data = stream.read(1024, exception_on_overflow=False)\n",
    "        frames.append(data)\n",
    "\n",
    "# Function to start recording audio\n",
    "def start_recording(button):\n",
    "    global recording, current_sentence_index, frames, recording_thread, stream\n",
    "    recording = True\n",
    "    button_start.disabled = True\n",
    "    button_stop.disabled = False\n",
    "    frames = []\n",
    "    sentence_text.value = sentences[current_sentence_index]\n",
    "    stream = p.open(format=pyaudio.paInt16,\n",
    "                channels=1,\n",
    "                rate=16000,\n",
    "                input=True,\n",
    "                frames_per_buffer=1024,\n",
    "                stream_callback=None)\n",
    "    stream.start_stream()\n",
    "    recording_thread = threading.Thread(target=record)\n",
    "    recording_thread.start()\n",
    "\n",
    "# Function to stop recording audio\n",
    "def stop_recording(button):\n",
    "    global recording, current_sentence_index, recording_thread, stream\n",
    "    recording = False\n",
    "    recording_thread.join()\n",
    "    button_start.disabled = False\n",
    "    button_stop.disabled = True\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    sentence_text.value = \"Recording complete.\"\n",
    "    write_audio(frames, f'fine_tuning_data/recording_sentence_{current_sentence_index+1}.wav')\n",
    "\n",
    "# Function to write audio frames to wav file\n",
    "def write_audio(frames, filename):\n",
    "    p = pyaudio.PyAudio()\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(1)\n",
    "    wf.setsampwidth(p.get_sample_size(pyaudio.paInt16))\n",
    "    wf.setframerate(16000)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "# Function to move to next sentence\n",
    "def next_sentence(button):\n",
    "    global current_sentence_index\n",
    "    if current_sentence_index < len(sentences) - 1:\n",
    "        current_sentence_index += 1\n",
    "        sentence_text.value = sentences[current_sentence_index]\n",
    "    else:\n",
    "        sentence_text.value = \"All sentences recorded.\"\n",
    "\n",
    "# List of sentences\n",
    "sentences = text_sents\n",
    "current_sentence_index = 0\n",
    "\n",
    "# Create buttons\n",
    "button_start = widgets.Button(description=\"Start Recording\")\n",
    "button_stop = widgets.Button(description=\"Stop Recording\")\n",
    "button_stop.disabled = True\n",
    "button_next = widgets.Button(description=\"Next\")\n",
    "button_next.disabled = False\n",
    "\n",
    "# Add button click event handlers\n",
    "button_start.on_click(start_recording)\n",
    "button_stop.on_click(stop_recording)\n",
    "button_next.on_click(next_sentence)\n",
    "\n",
    "# Create sentence text widget\n",
    "sentence_text = widgets.Textarea(value=sentences[current_sentence_index],\n",
    "                                  description='Sentence:',\n",
    "                                  layout={'height': '100px'}) \n",
    "# Display widgets\n",
    "display(sentence_text)\n",
    "display(button_start)\n",
    "display(button_stop)\n",
    "display(button_next)\n",
    "\n",
    "recording = False\n",
    "\n",
    "# Create PyAudio stream\n",
    "p = pyaudio.PyAudio()\n",
    "frames = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d323a99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob('fine_tuning_data/recording_sentence_*.wav')\n",
    "\n",
    "with open('fine_tuning_data/train.txt', 'w') as f:\n",
    "    f.write('path|transcript|duration\\n')\n",
    "    for fname in files:\n",
    "        index = int(fname.split('_')[-1].split('.')[0])\n",
    "        y, sr = librosa.load(fname)\n",
    "        duration = len(y) / sr\n",
    "        f.write(f'{fname}|{text_sents[index - 1]}|{duration}\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df332672",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76273653",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_error_rate(ref, hyp):\n",
    "    \"\"\"\n",
    "    Computes Word Error Rate (WER) while ignoring punctuation, even if it occurs within a word.\n",
    "\n",
    "    Args:\n",
    "        ref (str): Reference string\n",
    "        hyp (str): Hypothesis string\n",
    "\n",
    "    Returns:\n",
    "        float: Word Error Rate (WER) normalized to [0, 1]\n",
    "    \"\"\"\n",
    "    # Remove punctuation from reference and hypothesis strings\n",
    "    ref = ref.translate(str.maketrans('', '', string.punctuation))\n",
    "    hyp = hyp.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "    # Split reference and hypothesis into words\n",
    "    ref_words = ref.split()\n",
    "    hyp_words = hyp.split()\n",
    "\n",
    "    # Compute WER\n",
    "    num_errors = len(set(ref_words) ^ set(hyp_words)) # XOR to get unique words\n",
    "    wer = float(num_errors) / len(ref_words)\n",
    "\n",
    "    return wer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960a08e1",
   "metadata": {},
   "source": [
    "### Evaluate Vosk Transcriber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cbd59ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOG (VoskAPI:ReadDataFiles():model.cc:213) Decoding params beam=13 max-active=7000 lattice-beam=6\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:216) Silence phones 1:2:3:4:5:11:12:13:14:15\n",
      "LOG (VoskAPI:RemoveOrphanNodes():nnet-nnet.cc:948) Removed 0 orphan nodes.\n",
      "LOG (VoskAPI:RemoveOrphanComponents():nnet-nnet.cc:847) Removing 0 orphan components.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:248) Loading i-vector extractor from vosk-model-en-us-0.22/ivector/final.ie\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:183) Computing derived variables for iVector extractor\n",
      "LOG (VoskAPI:ComputeDerivedVars():ivector-extractor.cc:204) Done.\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:279) Loading HCLG from vosk-model-en-us-0.22/graph/HCLG.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:294) Loading words from vosk-model-en-us-0.22/graph/words.txt\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:303) Loading winfo vosk-model-en-us-0.22/graph/phones/word_boundary.int\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:310) Loading subtract G.fst model from vosk-model-en-us-0.22/rescore/G.fst\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:312) Loading CARPA model from vosk-model-en-us-0.22/rescore/G.carpa\n",
      "LOG (VoskAPI:ReadDataFiles():model.cc:318) Loading RNNLM model from vosk-model-en-us-0.22/rnnlm/final.raw\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d3544916a7a4234a132883d75460939",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.6804708503474562"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from source.speech_to_text.pretrained_vosk import PretrainedVoskTranscriber\n",
    "\n",
    "\n",
    "transcriber = PretrainedVoskTranscriber(\n",
    "    model_path='vosk-model-en-us-0.22',\n",
    "    sampling_rate=22050\n",
    ")\n",
    "\n",
    "word_error_rates = []\n",
    "    \n",
    "for fname in tqdm(files):\n",
    "    index = int(fname.split('_')[-1].split('.')[0])\n",
    "    with wave.open(fname, 'rb') as wf:\n",
    "        buffer = wf.readframes(wf.getnframes())\n",
    "        actual = transcriber.transcribe(buffer)\n",
    "    expected = text_sents[index - 1]\n",
    "    word_error_rates.append(word_error_rate(expected, actual))\n",
    "np.mean(np.array(word_error_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "171e3dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bed2087301e4830ad40910fee469111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.3513062611027567"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from source.speech_to_text.pretrained_whisper import PretrainedWhisperTranscriber\n",
    "\n",
    "\n",
    "transcriber = PretrainedWhisperTranscriber()\n",
    "\n",
    "for fname in tqdm(files):\n",
    "    index = int(fname.split('_')[-1].split('.')[0])\n",
    "    y, sr = sf.read(fname, dtype='float32')\n",
    "    y = librosa.resample(y, sr, target_sr=16000)\n",
    "    actual = transcriber.transcribe(y)\n",
    "    expected = text_sents[index - 1]\n",
    "    word_error_rates.append(word_error_rate(expected, actual))\n",
    "np.mean(np.array(word_error_rates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af06f3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
